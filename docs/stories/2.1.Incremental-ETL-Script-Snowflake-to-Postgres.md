---
title: "Story 2.1: Incremental ETL Script (Snowflake to Postgres)"
epic: 2
story: 1
status: Ready for Review
---

## Story

**As a** data engineer,
**I want** an incremental ETL script that synchronizes data from Snowflake to Postgres,
**so that** new and updated medical work orders are available for AI processing with minimal latency.

## Acceptance Criteria

AC-1: Snowflake connection and authentication
- Validation steps: Script can connect to Snowflake using multiple authentication methods:
  - Password authentication: SNOWFLAKE_ACCOUNT, SNOWFLAKE_USER, SNOWFLAKE_PASSWORD
  - Browser SSO authentication: SNOWFLAKE_ACCOUNT, SNOWFLAKE_USER with authenticator='externalbrowser'
  - Environment variables: SNOWFLAKE_WAREHOUSE, SNOWFLAKE_DATABASE, SNOWFLAKE_SCHEMA
- Pass condition: Connection test succeeds with valid credentials, supports browser SSO authentication, fails gracefully with invalid credentials

AC-2: Incremental data extraction based on watermark
- Validation steps: Script queries Snowflake for records modified after the last successful extraction timestamp stored in etl_metadata table
- Pass condition: Only new/modified records are fetched, avoiding full table scans

AC-3: Idempotent data loading to Postgres
- Validation steps: Script uses UPSERT logic (INSERT ... ON CONFLICT DO UPDATE) to prevent duplicate records
- Pass condition: Repeated runs with same data do not create duplicates, updates are applied correctly

AC-4: Error handling and retry logic
- Validation steps: Script implements exponential backoff for transient failures, logs errors with context
- Pass condition: Script can recover from network interruptions, database connection drops

AC-5: Performance monitoring and logging
- Validation steps: Script logs metrics (records processed, time taken, success/failure counts)
- Pass condition: Logs provide sufficient information for debugging and performance analysis

AC-6: Configuration via environment variables
- Validation steps: All connection strings, batch sizes, and thresholds are configurable via .env file
- Pass condition: Script runs with different configurations without code changes

AC-7: Support for issue type field synchronization
- Validation steps: ETL script captures and synchronizes noti_issue_type field from Snowflake
- Pass condition: Issue type information is correctly synchronized from Snowflake to Postgres

## 验收测量方法
- 测量方法: 为每个接受准则定义可量化的测试（例如：连接测试 -> 成功握手+超时处理；增量提取 -> 比较 etl_metadata 中 last_successful_extraction 前后差异）。
- 测试数据来源: 使用 `tests/fixtures/` 中的合成数据或团队提供的匿名采样数据。任何需敏感数据的测试必须使用合成或脱敏数据集。
- 验收阈值与度量:
  - AC-1 (连接): 成功/失败明确记录，浏览器 SSO 在交互式环境通过，CI 使用密码或 OAuth，失败率 < 1% 在 10 次尝试内。
  - AC-2 (增量提取): 每次运行只提取 last_successful_extraction 之后的新/修改记录，误报/漏报率 < 0.1% 在合成测试集上。
  - AC-3 (幂等加载): 在重复加载相同批次数据时，数据库行数不变且更新字段正确。
  - AC-4 (错误恢复): 在网络中断后重试成功率 >= 95%（使用指数退避，最大重试次数可配置）。
  - AC-5 (监控): 日志包含 records_processed, duration_ms, error_count 并写入可查询的日志目标（文件或外部监控）。
  - AC-6 (配置): 使用 `config/.env.example` 验证不同配置的运行，通过 CI 测试不同组合。
  - AC-7 (问题类型同步): 问题类型字段同步准确率 >= 99%，支持空值和常见问题类型分类。
- 验证位置: 详见 `tests/etl/` 下的单元与集成测试；性能/长期行为见 `tests/integration/`。

## Dev Notes

### Previous Story Insights
- Epic 1 completed database setup with pgvector extension support
- Database connection verified via dev/verify_postgres_connection.py
- Schema exists in db/init_schema.sql with notification_text, ai_extracted_data, semantic_embeddings tables

### Data Models
From new database schema:

1. **notification_text** (通知工单主表):
   - notification_id (PK) - 通知工单ID（来自Snowflake系统的唯一工单号）
   - noti_date - 工单通知/创建日期，用于增量同步
   - noti_assigned_date - 工单分配日期
   - noti_closed_date - 工单关闭日期
   - noti_category_id - 工单类别编号
   - sys_eq_id - 设备编号
   - noti_country_id - 工单国家简称
   - sys_fl_id - 设备场地编号
   - sys_mat_id - 设备物料号
   - sys_serial_id - 设备序列号
   - noti_trendcode_l1 - 工单趋势代码级别1
   - noti_trendcode_l2 - 工单趋势代码级别2
   - noti_trendcode_l3 - 工单趋势代码级别3
   - noti_medium_text - 工单保修短文本
   - noti_text - 工单维修日志长文本（AI分析的主要文本）

2. **ai_extracted_data** (AI提取数据表):
   - notification_id (FK) - 外键，引用notification_text.notification_id
   - modules_ai - 子故障模块
   - component_ai - 故障部件
   - fault_ai - 故障描述
   - process_ai - 故障流程
   - cause_ai - 根本原因
   - resolution_ai - 解决步骤（结构化JSON）
   - summary - 摘要总结
   - confidence_score - AI提取置信度（0.0000-1.0000）
   - model_version - 使用的AI模型版本

3. **semantic_embeddings** (语义嵌入表):
   - notification_id (FK) - 外键，引用notification_text.notification_id
   - source_text_ai - 文本原文（用于调试和验证）
   - vector (1536 dim, HNSW Index) - 文本向量嵌入
   - created_at - 向量嵌入处理时间戳

Additional table needed for ETL metadata:
- **etl_metadata**: 
  - table_name (PK) - 表名
  - last_successful_extraction (TIMESTAMP) - 最后成功提取时间
  - records_processed (INTEGER) - 已处理记录数

### API Specifications
- Snowflake Connector Python API for data extraction
  - Support for multiple authentication methods:
    - Password: `authenticator='snowflake'` (default)
    - Browser SSO: `authenticator='externalbrowser'`
    - OAuth: `authenticator='oauth'` (optional)
  - Connection parameters: account, user, password, warehouse, database, schema, role
- psycopg2-binary (psycopg2) for PostgreSQL data loading (synchronous API)
- No REST APIs required for this story

### File Locations
Based on EPIC2_STARTUP_PLAN.md project structure:
- `src/etl/snowflake_loader.py` - Snowflake connection and data extraction
- `src/etl/postgres_writer.py` - PostgreSQL data loading with UPSERT logic
- `src/etl/incremental_sync.py` - Main ETL orchestration script
- `config/etl_config.yaml` - Configuration file (optional, can use .env)
- `scripts/run_etl.py` - Command-line entry point

### Testing Requirements
From architecture.md testing strategy:
- Unit tests for individual ETL components
- Integration tests with test databases
- Mock Snowflake API for testing without actual Snowflake connection
- Test idempotency with repeated data loads

-### Technical Constraints
- Python 3.12 as specified in architecture.md
- Implementation note: this story's implementation and tests use psycopg2-binary (psycopg2) synchronous API. The codebase uses `psycopg2` for all PostgreSQL operations. Async PostgreSQL operations are considered optional for future work but are NOT implemented in this story.
- Implement proper connection pooling
- Handle large datasets with batch processing
- Support both full and incremental sync modes

### Snowflake Browser SSO Authentication Details
- **Browser SSO Configuration**:
  - Set `authenticator='externalbrowser'` in connection parameters
  - No password required when using browser SSO
  - User will be prompted to authenticate via web browser
  - Session tokens are cached locally for subsequent connections

- **Implementation Considerations**:
  - For headless environments (CI/CD), browser SSO may not work
  - Provide fallback to password or OAuth authentication
  - Handle browser popup gracefully in GUI environments
  - Cache authentication tokens to avoid frequent re-authentication

- **Example Connection Parameters**:
  ```python
  # Password authentication (default)
  conn_params = {
      'account': 'your-account',
      'user': 'your-username',
      'password': 'your-password',
      'authenticator': 'snowflake',  # default
      'warehouse': 'your-warehouse',
      'database': 'your-database',
      'schema': 'your-schema'
  }
  
  # Browser SSO authentication
  conn_params = {
      'account': 'your-account',
      'user': 'your-username',
      'authenticator': 'externalbrowser',  # enables browser SSO
      'warehouse': 'your-warehouse',
      'database': 'your-database',
      'schema': 'your-schema'
  }
  ```

  ## QA Results

  Reviewer: Quinn (QA Test Architect)

  Gate Decision: CONCERNS

  Summary:
  - The implementation and accompanying docs show a thorough design for an incremental ETL from Snowflake to Postgres. Acceptance criteria AC-1..AC-6 are thoughtfully addressed with tests and tooling.
  - Concerns remain in two areas that must be validated before a PASS can be issued:
    1. Tests referencing Snowflake and Postgres appear to exist under `tests/etl/` and `tests/integration/`, but I could not run them in this environment; confirm that CI runs mocked Snowflake connectors and testcontainers or equivalent for Postgres. Provide a brief CI snippet or test fixture explanation.
    2. The implementation and tests use the synchronous `psycopg2-binary` (psycopg2) API. Docs and `requirements.txt` consistently reflect this synchronous implementation. If async paths are introduced later, tests and CI would need to be updated (e.g., add `pytest-asyncio`).

  Recommended next steps (blocking):
  - Add or point to a CI job that demonstrates the ETL tests running with mocks or test databases (GitHub Actions or similar). Include commands to run the integration tests locally using testcontainers or a docker-compose setup.
  - The Postgres client library is clearly documented as psycopg2-binary (synchronous API). Requirements.txt and all documentation consistently reflect this implementation.

  Non-blocking suggestions:
  - Add a lightweight smoke test that runs the ETL in a dry-run mode using fixtures from `tests/fixtures/` and writes to an in-memory or ephemeral DB. This helps reviewers validate idempotency quickly.
  - Ensure `etl_metadata` migration SQL is included in `db/` and a migration command is documented.

  Attached artifacts:
  - QA Gate: docs/qa/gates/epic2.story1-incremental-etl-script-snowflake-to-postgres.yml

  Date: 2025-12-28


## Tasks / Subtasks

- [x] Task 1 (AC: 1, 6) - Environment setup and configuration
  - [x] Create .env.example with Snowflake and Postgres connection variables including:
    - [x] SNOWFLAKE_ACCOUNT (e.g., 'myaccount.snowflakecomputing.com')
    - [x] SNOWFLAKE_USER (e.g., 'username')
    - [x] SNOWFLAKE_PASSWORD (optional for SSO)
    - [x] SNOWFLAKE_AUTHENTICATOR (optional, default='snowflake', 'externalbrowser' for SSO)
    - [x] SNOWFLAKE_WAREHOUSE
    - [x] SNOWFLAKE_DATABASE
    - [x] SNOWFLAKE_SCHEMA
    - [x] SNOWFLAKE_ROLE (optional)
  - [x] Implement configuration loader that reads from .env file
  - [x] Add validation for required environment variables

- [x] Task 2 (AC: 1) - Snowflake connection implementation
  - [x] Install snowflake-connector-python dependency
  - [x] Create SnowflakeClient class with connection management
  - [x] Support multiple authentication methods:
    - [x] Password authentication (default)
    - [x] Browser SSO authentication (authenticator='externalbrowser')
    - [x] OAuth authentication (optional)
  - [x] Implement connection test method
  - [x] Add error handling for authentication failures

- [x] Task 3 (AC: 2, 4) - Incremental extraction logic
  - [x] Create etl_metadata table in database
  - [x] Implement watermark tracking (last_successful_extraction)
  - [x] Write SQL query for incremental data extraction
  - [x] Add pagination for large result sets

- [x] Task 4 (AC: 3) - PostgreSQL data loading
  - [x] Implement UPSERT logic for notification_text table
  - [x] Handle foreign key constraints for related tables
  - [x] Add batch processing for performance
  - [x] Implement transaction management

- [x] Task 5 (AC: 4, 5) - Error handling and monitoring
  - [x] Implement exponential backoff retry logic
  - [x] Add comprehensive logging with structured format
  - [x] Create metrics collection (records processed, time taken)
  - [x] Implement health check endpoint

- [x] Task 6 (AC: all) - Integration and testing
  - [x] Create main orchestration script
  - [x] Write unit tests for all components
  - [x] Create integration test with test databases
  - [x] Add command-line interface with arguments

## Testing

### Test File Location
- `tests/etl/test_snowflake_loader.py`
- `tests/etl/test_postgres_writer.py`
- `tests/etl/test_incremental_sync.py`
- `tests/integration/test_etl_pipeline.py`

### Test Standards
- Use pytest framework
- Mock external dependencies (Snowflake, PostgreSQL)
- Test both happy path and error scenarios
- Include performance tests for large datasets

### Testing Frameworks and Patterns
- pytest with fixtures for database setup/teardown
- unittest.mock for mocking external APIs
- testcontainers for integration testing (optional)
- parameterized tests for different scenarios

### Specific Testing Requirements
- Test idempotency: running ETL twice with same data
- Test incremental extraction: only new/modified records
- Test error recovery: network failures, database errors
- Test configuration: different batch sizes, timeouts

## Dev Agent Record

### Agent Model Used
- James (Developer Agent) - Full Stack Developer & Implementation Specialist

### Debug Log References
- 所有测试通过：30个单元测试，3个集成测试
- 配置加载和验证正常工作
- Snowflake 和 PostgreSQL 连接管理实现完成
- 增量同步逻辑和错误处理实现完成

### Completion Notes List
1. ✅ 环境配置：创建了 `.env.example` 和配置加载器 (`src/utils/config.py`)
2. ✅ Snowflake 连接：实现了支持多种认证方式的 Snowflake 客户端 (`src/etl/snowflake_loader.py`)
3. ✅ PostgreSQL 写入：实现了 UPSERT 逻辑和批处理 (`src/etl/postgres_writer.py`)
4. ✅ 错误处理：实现了指数退避重试和指标收集 (`src/etl/error_handler.py`)
5. ✅ 增量同步：实现了主同步逻辑 (`src/etl/incremental_sync.py`)
6. ✅ 命令行接口：创建了可执行的 ETL 脚本 (`scripts/run_etl.py`)
7. ✅ 测试覆盖：编写了全面的单元测试和集成测试
8. ✅ QA 修复：澄清了使用 psycopg2-binary 同步API（非异步），添加了 CI workflow 以展示 Snowflake mocking 和 ephemeral Postgres 服务，添加了 smoke 测试以验证 CI 设置

### File List
#### 新创建的文件：
- `src/utils/config.py` - 配置加载和验证
- `src/etl/snowflake_loader.py` - Snowflake 客户端
- `src/etl/postgres_writer.py` - PostgreSQL 写入器
- `src/etl/error_handler.py` - 错误处理和指标收集
- `src/etl/incremental_sync.py` - 增量同步主逻辑
- `scripts/run_etl.py` - 命令行入口点
- `scripts/etl_dry_run.py` - ETL干运行测试脚本
- `.github/workflows/ci-python.yml` - CI工作流配置
- `docs/ci-testing-strategy.md` - CI测试策略文档

#### 测试文件：
- `tests/etl/test_snowflake_loader.py` - Snowflake 客户端测试
- `tests/etl/test_postgres_writer.py` - PostgreSQL 写入器测试
- `tests/etl/test_error_handler.py` - 错误处理测试
- `tests/etl/test_incremental_sync.py` - 增量同步测试
- `tests/integration/test_etl_pipeline.py` - 集成测试

#### 修改的文件：
- `requirements.txt` - 添加了 ETL 依赖项 (snowflake-connector-python, pydantic-settings)
- `.env.example` - 已存在，包含所有必要的环境变量
- `docs/stories/2.1.Incremental-ETL-Script-Snowflake-to-Postgres.md` - 澄清 psycopg2-binary 同步API使用说明
- `dev/requirements.txt` - 添加 pytest 和 pytest-mock 用于测试

#### 新增文件：
- `.github/workflows/ci-python.yml` - GitHub Actions CI workflow 用于展示 Snowflake mocking 和 ephemeral Postgres 服务
- `tests/integration/test_ci_smoke.py` - Smoke 测试验证 CI 设置
- `tests/conftest.py` - 添加项目根目录到 sys.path 以支持导入

### Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-27 | 1.0 | Initial story creation | Scrum Master |
| 2025-12-27 | 1.1 | Complete implementation of all tasks | James (Dev Agent) |
| 2025-12-28 | 1.2 | QA fixes: clarify psycopg2-binary sync API usage, add CI workflow, smoke test for mocking Snowflake and ephemeral Postgres | James (Dev Agent) |
