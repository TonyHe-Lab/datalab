---
title: "Story 1.3: Database Schema & Extension Validation (Ensure vector extension exists)"
epic: 1
story: 3
status: Approved
---

## Story

**As a** backend developer,
**I want** the Postgres schema created and validated (including vector extension and required tables/indexes),
**so that** the application can store raw logs, structured AI-extracted data, and semantic embeddings required for hybrid search.

## Acceptance Criteria

1. Required Postgres extensions (e.g., vector) are present or created if permitted.
2. Tables `maintenance_logs`, `ai_extracted_data`, and `semantic_embeddings` exist with expected columns and constraints.
3. Indexes for efficient text search and vector similarity (HNSW if available) are created and verified.
4. A migration or SQL script `db/init_schema.sql` is provided to create/validate schema idempotently.
5. A small verification script `dev/verify_db_schema.py` confirms table existence, extension availability, and basic insert/query operations.

## Dev Notes

Data Model Summary:
- `maintenance_logs`: id PK, snowflake_id Unique, raw_text, last_modified. [Source: docs/prd_shards/10-data-model.md]
- `ai_extracted_data`: log_id FK, component, fault, cause, resolution_steps (Text/JSON), summary. [Source: docs/prd_shards/10-data-model.md]
- `semantic_embeddings`: log_id FK, vector (e.g., 1536 dim), compatible with HNSW index. [Source: docs/prd_shards/10-data-model.md]

Architecture Guidance:
- If vector extension (pgvector) is used, ensure extension installed and the `vector` column uses appropriate dimension (e.g., 1536) matching embedding model. [Source: docs/prd_shards/09-technical-implementations.md]
- Provide fallback guidance if vector extension not available: store embeddings as bytea or JSON and log limitation in story. [Source: docs/prd_shards/10-data-model.md]

WSL/Host Connectivity Note:
- Verification scripts should honor DB_HOST resolution strategy used in the project (see `/etc/resolv.conf` parsing guidance). [Source: docs/prd_shards/09-technical-implementations.md]

Security:
- Migrations must not hardcode credentials. Use env-based connection strings and document required privileges to create extensions and schemas. [Source: docs/prd_shards/12-security.md]

## Tasks / Subtasks

- [ ] Task 1 (AC: 4) — Add migration SQL
  - [ ] Create `db/init_schema.sql` that creates extensions conditionally (e.g., `CREATE EXTENSION IF NOT EXISTS vector;`) and creates tables with IF NOT EXISTS.
  - [ ] Add comments explaining each table/column and expected types.

Suggested `db/init_schema.sql` snippets (include these examples in the story for developer copy-paste):

```sql
-- Create extension if allowed (requires superuser)
CREATE EXTENSION IF NOT EXISTS vector;

-- maintenance_logs
CREATE TABLE IF NOT EXISTS maintenance_logs (
  id SERIAL PRIMARY KEY,
  snowflake_id TEXT UNIQUE,
  raw_text TEXT NOT NULL,
  last_modified TIMESTAMP WITH TIME ZONE DEFAULT now()
);

-- ai_extracted_data
CREATE TABLE IF NOT EXISTS ai_extracted_data (
  id SERIAL PRIMARY KEY,
  log_id INTEGER NOT NULL REFERENCES maintenance_logs(id) ON DELETE CASCADE,
  component TEXT,
  fault TEXT,
  cause TEXT,
  resolution_steps JSONB,
  summary TEXT
);

-- semantic_embeddings (using pgvector)
CREATE TABLE IF NOT EXISTS semantic_embeddings (
  log_id INTEGER NOT NULL REFERENCES maintenance_logs(id) ON DELETE CASCADE,
  vector vector(1536),
  PRIMARY KEY (log_id)
);

-- Example index (ivfflat) — adjust depending on extension capabilities
CREATE INDEX IF NOT EXISTS semantic_embeddings_vector_idx ON semantic_embeddings USING ivfflat (vector) WITH (lists = 100);
```

Suggested `dev/verify_db_schema.py` minimal implementation:

```python
import os
import sys
import psycopg2

DATABASE_URL = os.getenv('DATABASE_URL') or os.getenv('POSTGRES_URL')
if not DATABASE_URL:
    print('DATABASE_URL not set')
    sys.exit(2)

conn = psycopg2.connect(DATABASE_URL)
cur = conn.cursor()
cur.execute("SELECT to_regclass('public.maintenance_logs')")
print('maintenance_logs exists:', cur.fetchone())
cur.execute("SELECT exists(SELECT 1 FROM pg_extension WHERE extname='vector')")
print('vector extension present:', cur.fetchone())
conn.close()
print('verify_db_schema completed')
```


- [ ] Task 2 (AC: 1,3) — Validate vector extension & indexes
  - [ ] Add SQL statements to check extension existence and create HNSW/GIST indexes where supported (e.g., `CREATE INDEX ON semantic_embeddings USING ivfflat (vector);` or `CREATE INDEX ON semantic_embeddings USING hnsw (vector);` depending on extension).
  - [ ] Provide fallback GIN or GiST indexes for text columns for keyword search.

- [ ] Task 3 (AC: 5) — Verification scripts
  - [ ] Create `dev/verify_db_schema.py` to connect using env vars, assert existence of tables and extensions, attempt a sample insert/read, and verify vector operations if possible.
  - [ ] Document how CI can run verification with ephemeral DB or connection to dev DB.

- [ ] Task 4 (AC: 2) — Document migration usage & permissions
  - [ ] Add `docs/setup/db-schema.md` explaining how to run `db/init_schema.sql`, required DB roles/privileges, and rollback steps.

## Testing

- Verification script performs basic schema checks and simple hybrid search sample (if vector extension available). Include exit codes for CI.

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|

## Dev Agent Record

*To be populated by the development agent during implementation.*

## QA Results

*To be populated by QA agent.*
