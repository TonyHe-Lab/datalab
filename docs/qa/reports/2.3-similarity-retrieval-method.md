---
title: "Story 2.3 Similarity Retrieval Regression: Method & Results"
date: 2025-12-28
author: Dev Agent
---

Overview
- This document describes the small regression test for embedding similarity behavior (AC‑4).

Dataset
- Queries: 3 short maintenance‑related phrases.
- Corpus: 5 synthetic maintenance sentences.

Procedure
1. Mock Azure OpenAI embeddings to return deterministic pseudo‑random unit vectors (seed based on text).
2. Generate embeddings for queries and corpus using `EmbeddingPipeline.batch_generate()`.
3. Compute cosine similarity between each query and each corpus vector.
4. Evaluate whether each query's highest similarity corresponds to the semantically matching corpus entry (by index).
5. Compute Top‑1 accuracy.

Thresholds
- No strict threshold; the test ensures embeddings are generated, have correct dimension (1536), and similarity computation works.
- In real deployment, semantic similarity should be validated with human‑labeled pairs.

Results (from test run)
- Embedding dimension: 1536 for all vectors.
- Similarity matrix computed.
- Top‑1 accuracy: varies with random vectors; expectation is that deterministic vectors produce consistent matches.

Interpretation
- This regression test is a sanity check for the embedding pipeline, not a measure of embedding quality.
- For production, consider evaluating with labeled datasets (e.g., MRR, nDCG, Top‑K hit rate).

Reproduction
- Execute test: `pytest tests/integration/test_similarity_retrieval.py -q -s`
- The test prints the similarity matrix and accuracy.

Notes
- Real Azure embeddings will have semantic properties; this mock only tests the pipeline mechanics.
- Future work: add a small labeled dataset and compute retrieval metrics.
